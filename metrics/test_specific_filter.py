#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•ç‰¹å®šæ•°æ®è¿‡æ»¤å™¨
æ ¹æ®ä¸åŒçš„æµ‹è¯•ç±»å‹ï¼Œè‡ªåŠ¨è¿‡æ»¤æ‰æ— æ•ˆçš„æ•°æ®æŒ‡æ ‡
ä½œè€…: Jaxon
æ—¥æœŸ: 2025-01-01
"""

import json
import os
import glob
from pathlib import Path
from typing import List, Dict, Any, Set
from datetime import datetime
from rich.console import Console

console = Console()

class TestSpecificFilter:
    """æµ‹è¯•ç‰¹å®šæ•°æ®è¿‡æ»¤å™¨"""
    
    def __init__(self):
        self.console = Console()
        
        # å®šä¹‰æ¯ç§æµ‹è¯•ç±»å‹çš„æ— æ•ˆæ•°æ®è§„åˆ™
        self.test_specific_rules = {
            "åä¸ºäº‘è¿æ¥æµ‹è¯•": {
                "invalid_metrics": [
                    # è¿æ¥æµ‹è¯•ä¸­pubç›¸å…³çš„æŒ‡æ ‡æ— æ„ä¹‰
                    "pub_fail", "pub_overrun", "pub_succ", "pub",
                    # è®¢é˜…ç›¸å…³çš„æŒ‡æ ‡æ— æ„ä¹‰
                    "sub_fail", "sub", "reconnect_succ",
                    # å‘å¸ƒå»¶è¿Ÿåœ¨è¿æ¥æµ‹è¯•ä¸­æ— æ„ä¹‰
                    "publish_latency"
                ],
                "keep_metrics": [
                    "connect_succ", "connect_fail", "connect_retried",
                    "connection_timeout", "connection_refused", "unreachable",
                    "connection_idle", "recv"
                ]
            },
            "åä¸ºäº‘å‘å¸ƒæµ‹è¯•": {
                "invalid_metrics": [
                    # å‘å¸ƒæµ‹è¯•ä¸­è®¢é˜…ç›¸å…³çš„æŒ‡æ ‡æ— æ„ä¹‰
                    "sub_fail", "sub", "reconnect_succ",
                    # è¿æ¥é‡è¯•åœ¨å‘å¸ƒæµ‹è¯•ä¸­é€šå¸¸æ— æ„ä¹‰
                    "connect_retried"
                ],
                "keep_metrics": [
                    "pub_succ", "pub_fail", "pub_overrun", "pub",
                    "publish_latency", "connect_succ", "connect_fail",
                    "connection_timeout", "connection_refused", "unreachable",
                    "connection_idle", "recv"
                ]
            },
            "åä¸ºäº‘è®¢é˜…æµ‹è¯•": {
                "invalid_metrics": [
                    # è®¢é˜…æµ‹è¯•ä¸­å‘å¸ƒç›¸å…³çš„æŒ‡æ ‡æ— æ„ä¹‰
                    "pub_fail", "pub_overrun", "pub_succ", "pub",
                    "publish_latency"
                ],
                "keep_metrics": [
                    "sub_fail", "sub", "reconnect_succ",
                    "connect_succ", "connect_fail", "connect_retried",
                    "connection_timeout", "connection_refused", "unreachable",
                    "connection_idle", "recv"
                ]
            },
            "åä¸ºäº‘å¹¿æ’­æµ‹è¯•": {
                "invalid_metrics": [
                    # å¹¿æ’­æµ‹è¯•ä¸­æŸäº›ç‰¹å®šæŒ‡æ ‡å¯èƒ½æ— æ„ä¹‰
                    "connect_retried"  # å¹¿æ’­æµ‹è¯•é€šå¸¸ä¸éœ€è¦é‡è¿
                ],
                "keep_metrics": [
                    "pub_succ", "pub_fail", "pub_overrun", "pub",
                    "publish_latency", "sub_fail", "sub", "reconnect_succ",
                    "connect_succ", "connect_fail",
                    "connection_timeout", "connection_refused", "unreachable",
                    "connection_idle", "recv"
                ]
            }
        }
        
        # é€šç”¨æ— æ•ˆæ•°æ®è§„åˆ™
        self.common_invalid_patterns = {
            # Erlang VMç³»ç»ŸæŒ‡æ ‡ï¼ˆä¸MQTTæµ‹è¯•æ€§èƒ½æ— å…³ï¼‰
            'erlang_vm_metrics': [
                'erlang_vm_memory_', 'erlang_vm_msacc_', 'erlang_vm_statistics_',
                'erlang_vm_dirty_', 'erlang_vm_ets_', 'erlang_vm_logical_',
                'erlang_vm_port_', 'erlang_vm_process_', 'erlang_vm_schedulers',
                'erlang_vm_smp_', 'erlang_vm_threads', 'erlang_vm_time_',
                'erlang_vm_wordsize_', 'erlang_vm_atom_', 'erlang_vm_allocators',
                'erlang_vm_thread_pool_size', 'erlang_vm_thread_pool_'
            ],
            # ç›´æ–¹å›¾æ¡¶æ•°æ®ï¼ˆé€šå¸¸åŒ…å«å¤§é‡é›¶å€¼æ¡¶ï¼‰
            'histogram_buckets': [
                '_bucket', '_count', '_sum'
            ],
            # é‡å¤çš„help_text
            'redundant_help_text': [
                'connection_idle connection_idle', 'recv recv', 'connect_fail connect_fail',
                'pub_fail pub_fail', 'pub_overrun pub_overrun', 'connect_retried connect_retried',
                'connect_succ connect_succ', 'sub_fail sub_fail', 'reconnect_succ reconnect_succ',
                'sub sub', 'publish_latency publish_latency', 'pub_succ pub_succ',
                'connection_timeout connection_timeout', 'connection_refused connection_refused',
                'unreachable unreachable', 'pub pub'
            ]
        }
    
    def filter_test_data(self, test_name: str, raw_metrics: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æ ¹æ®æµ‹è¯•ç±»å‹è¿‡æ»¤æ•°æ®"""
        if not raw_metrics:
            return []
        
        self.console.print(f"[blue]ğŸ” å¼€å§‹è¿‡æ»¤ {test_name} çš„æ— æ•ˆæ•°æ®...[/blue]")
        
        # è·å–æµ‹è¯•ç‰¹å®šçš„è¿‡æ»¤è§„åˆ™
        test_rules = self.test_specific_rules.get(test_name, {})
        invalid_metrics = test_rules.get("invalid_metrics", [])
        keep_metrics = test_rules.get("keep_metrics", [])
        
        filtered_metrics = []
        removed_count = 0
        removed_details = []
        
        for metric in raw_metrics:
            metric_name = metric.get('name', '')
            metric_value = metric.get('value', 0)
            help_text = metric.get('help_text', '')
            
            # æ£€æŸ¥æ˜¯å¦åº”è¯¥è¿‡æ»¤æ­¤æŒ‡æ ‡
            should_remove = False
            removal_reason = ""
            
            # 1. æ£€æŸ¥æµ‹è¯•ç‰¹å®šçš„æ— æ•ˆæŒ‡æ ‡
            if metric_name in invalid_metrics:
                should_remove = True
                removal_reason = f"æµ‹è¯•ç‰¹å®šæ— æ•ˆæŒ‡æ ‡ ({test_name})"
            
            # 2. æ£€æŸ¥Erlang VMç³»ç»ŸæŒ‡æ ‡
            elif any(metric_name.startswith(pattern) for pattern in self.common_invalid_patterns['erlang_vm_metrics']):
                should_remove = True
                removal_reason = "Erlang VMç³»ç»ŸæŒ‡æ ‡"
            
            # 3. æ£€æŸ¥ç›´æ–¹å›¾æ¡¶æ•°æ®ï¼ˆä¿ç•™æœ‰æ„ä¹‰çš„æ¡¶ï¼‰
            elif any(pattern in metric_name for pattern in self.common_invalid_patterns['histogram_buckets']):
                if metric_value == 0:
                    should_remove = True
                    removal_reason = "é›¶å€¼ç›´æ–¹å›¾æ¡¶"
            
            # 4. æ£€æŸ¥é‡å¤çš„help_text
            elif help_text in self.common_invalid_patterns['redundant_help_text']:
                should_remove = True
                removal_reason = "é‡å¤çš„help_text"
            
            # 5. æ£€æŸ¥é›¶å€¼ä¸”ä¸åœ¨ä¿ç•™åˆ—è¡¨ä¸­çš„æŒ‡æ ‡
            elif metric_value == 0 and metric_name not in keep_metrics:
                should_remove = True
                removal_reason = "é›¶å€¼ä¸”éå…³é”®æŒ‡æ ‡"
            
            # 6. æ£€æŸ¥æ˜¯å¦æœ‰å®é™…æ„ä¹‰çš„æŒ‡æ ‡
            if not should_remove:
                # ä¿ç•™å…³é”®æ€§èƒ½æŒ‡æ ‡
                key_metrics = [
                    'connect_succ', 'pub_succ', 'recv', 'publish_latency',
                    'mqtt_client_connect_duration', 'mqtt_client_handshake_duration',
                    'e2e_latency', 'mqtt_client_subscribe_duration'
                ]
                
                # å¦‚æœæ˜¯æŒ‡æ ‡åç§°åŒ…å«å…³é”®æ€§èƒ½æŒ‡æ ‡ï¼Œæˆ–è€…å€¼ä¸ä¸º0ï¼Œåˆ™ä¿ç•™
                if not (any(key_metric in metric_name for key_metric in key_metrics) or 
                       metric_value != 0 or 
                       'duration' in metric_name or 
                       'latency' in metric_name or
                       metric_name in keep_metrics):
                    should_remove = True
                    removal_reason = "æ— å®é™…æ„ä¹‰çš„æŒ‡æ ‡"
            
            if should_remove:
                removed_count += 1
                removed_details.append(f"{metric_name}: {removal_reason} (å€¼: {metric_value})")
                if removed_count <= 10:  # åªæ˜¾ç¤ºå‰10ä¸ªè¢«ç§»é™¤çš„æŒ‡æ ‡
                    self.console.print(f"[dim]  âŒ ç§»é™¤ {metric_name}: {removal_reason} (å€¼: {metric_value})[/dim]")
            else:
                filtered_metrics.append(metric)
        
        # æ˜¾ç¤ºè¿‡æ»¤ç»Ÿè®¡
        self.console.print(f"[green]âœ… æ•°æ®è¿‡æ»¤å®Œæˆ: ä¿ç•™ {len(filtered_metrics)} ä¸ªæŒ‡æ ‡ï¼Œç§»é™¤ {removed_count} ä¸ªæ— æ•ˆæŒ‡æ ‡[/green]")
        
        # æ˜¾ç¤ºè¿‡æ»¤è¯¦æƒ…ï¼ˆå¦‚æœç§»é™¤çš„æŒ‡æ ‡ä¸å¤šï¼‰
        if removed_count <= 20:
            self.console.print(f"[dim]ç§»é™¤çš„æŒ‡æ ‡è¯¦æƒ…:[/dim]")
            for detail in removed_details[:10]:
                self.console.print(f"[dim]  â€¢ {detail}[/dim]")
        
        return filtered_metrics
    
    def filter_continuous_metrics_file(self, file_path: str) -> str:
        """è¿‡æ»¤æŒç»­æŒ‡æ ‡æ–‡ä»¶"""
        try:
            self.console.print(f"[blue]ğŸ“Š å¤„ç†æ–‡ä»¶: {os.path.basename(file_path)}[/blue]")
            
            # è¯»å–åŸå§‹æ•°æ®
            with open(file_path, 'r', encoding='utf-8') as f:
                raw_data = json.load(f)
            
            if not raw_data:
                self.console.print(f"[yellow]âš ï¸ æ–‡ä»¶ä¸ºç©º: {file_path}[/yellow]")
                return None
            
            # è·å–æµ‹è¯•åç§°
            test_name = raw_data[0].get('test_name', 'Unknown') if raw_data else 'Unknown'
            
            # è¿‡æ»¤æ¯ä¸ªæ—¶é—´ç‚¹çš„æ•°æ®
            filtered_data = []
            total_original_metrics = 0
            total_filtered_metrics = 0
            
            for data_point in raw_data:
                metrics = data_point.get('metrics', [])
                total_original_metrics += len(metrics)
                
                # è¿‡æ»¤æŒ‡æ ‡
                filtered_metrics = self.filter_test_data(test_name, metrics)
                total_filtered_metrics += len(filtered_metrics)
                
                # åˆ›å»ºè¿‡æ»¤åçš„æ•°æ®ç‚¹
                filtered_point = data_point.copy()
                filtered_point['metrics'] = filtered_metrics
                filtered_point['filter_info'] = {
                    "original_count": len(metrics),
                    "filtered_count": len(filtered_metrics),
                    "removed_count": len(metrics) - len(filtered_metrics),
                    "filter_timestamp": datetime.now().isoformat()
                }
                filtered_data.append(filtered_point)
            
            # ç”Ÿæˆè¿‡æ»¤åçš„æ–‡ä»¶å
            base_name = os.path.basename(file_path)
            name_parts = base_name.split('_')
            if len(name_parts) >= 3:
                filtered_filename = f"filtered_{name_parts[0]}_{name_parts[1]}_{name_parts[2]}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            else:
                filtered_filename = f"filtered_{base_name}"
            
            # ä¿å­˜è¿‡æ»¤åçš„æ•°æ®
            # æ ¹æ®å½“å‰å·¥ä½œç›®å½•ç¡®å®šæ­£ç¡®çš„è·¯å¾„
            if os.path.basename(os.getcwd()) == "metrics":
                # å¦‚æœåœ¨metricsç›®å½•ä¸­è¿è¡Œï¼Œç›´æ¥ä½¿ç”¨reports/filtered
                filtered_path = os.path.join("reports", "filtered", filtered_filename)
            else:
                # å¦‚æœåœ¨é¡¹ç›®æ ¹ç›®å½•è¿è¡Œï¼Œä½¿ç”¨metrics/reports/filtered
                filtered_path = os.path.join("metrics", "reports", "filtered", filtered_filename)
            os.makedirs(os.path.dirname(filtered_path), exist_ok=True)
            
            with open(filtered_path, 'w', encoding='utf-8') as f:
                json.dump(filtered_data, f, indent=2, ensure_ascii=False)
            
            # æ˜¾ç¤ºè¿‡æ»¤ç»Ÿè®¡
            removed_count = total_original_metrics - total_filtered_metrics
            self.console.print(f"[green]âœ… æ–‡ä»¶è¿‡æ»¤å®Œæˆ: {os.path.basename(file_path)}[/green]")
            self.console.print(f"[dim]  â€¢ åŸå§‹æŒ‡æ ‡æ€»æ•°: {total_original_metrics}[/dim]")
            self.console.print(f"[dim]  â€¢ è¿‡æ»¤åæŒ‡æ ‡æ•°: {total_filtered_metrics}[/dim]")
            self.console.print(f"[dim]  â€¢ ç§»é™¤æŒ‡æ ‡æ•°: {removed_count}[/dim]")
            self.console.print(f"[dim]  â€¢ è¿‡æ»¤åæ–‡ä»¶: {filtered_path}[/dim]")
            
            return filtered_path
            
        except Exception as e:
            self.console.print(f"[red]âŒ è¿‡æ»¤æ–‡ä»¶å¤±è´¥ {file_path}: {e}[/red]")
            return None
    
    def auto_filter_all_continuous_files(self, reports_dir: str = None) -> List[str]:
        """è‡ªåŠ¨è¿‡æ»¤æ‰€æœ‰æŒç»­æŒ‡æ ‡æ–‡ä»¶"""
        self.console.print("[blue]ğŸ” å¼€å§‹è‡ªåŠ¨è¿‡æ»¤æ‰€æœ‰æŒç»­æŒ‡æ ‡æ–‡ä»¶...[/blue]")
        
        # æ ¹æ®å½“å‰å·¥ä½œç›®å½•ç¡®å®šæ­£ç¡®çš„reportsç›®å½•
        if reports_dir is None:
            if os.path.basename(os.getcwd()) == "metrics":
                reports_dir = "reports"
            else:
                reports_dir = "metrics/reports"
        
        # æŸ¥æ‰¾æ‰€æœ‰æŒç»­æŒ‡æ ‡æ–‡ä»¶
        pattern = os.path.join(reports_dir, "continuous_metrics_*.json")
        continuous_files = glob.glob(pattern)
        
        # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå°è¯•æŸ¥æ‰¾åŒ…å«ä¸­æ–‡çš„æ–‡ä»¶
        if not continuous_files:
            pattern = os.path.join(reports_dir, "continuous_metrics_*æµ‹è¯•*.json")
            continuous_files = glob.glob(pattern)
        
        if not continuous_files:
            self.console.print(f"[yellow]âš ï¸ æœªæ‰¾åˆ°æŒç»­æŒ‡æ ‡æ–‡ä»¶: {pattern}[/yellow]")
            return []
        
        self.console.print(f"[blue]ğŸ“ æ‰¾åˆ° {len(continuous_files)} ä¸ªæŒç»­æŒ‡æ ‡æ–‡ä»¶[/blue]")
        
        filtered_files = []
        total_original_metrics = 0
        total_filtered_metrics = 0
        
        for file_path in continuous_files:
            # æ£€æŸ¥æ˜¯å¦å·²ç»å­˜åœ¨è¿‡æ»¤åçš„æ–‡ä»¶
            base_name = os.path.basename(file_path)
            name_parts = base_name.split('_')
            if len(name_parts) >= 3:
                test_type = f"{name_parts[2]}_{name_parts[3]}" if len(name_parts) > 3 else name_parts[2]
                filtered_pattern = os.path.join("metrics", "reports", "filtered", f"filtered_continuous_metrics_{test_type}_*.json")
                existing_files = glob.glob(filtered_pattern)
                
                if existing_files:
                    self.console.print(f"[yellow]âš ï¸ è¿‡æ»¤æ–‡ä»¶å·²å­˜åœ¨: {existing_files[0]}[/yellow]")
                    self.console.print(f"[dim]è·³è¿‡é‡å¤è¿‡æ»¤: {base_name}[/dim]")
                    continue
            
            # è¿‡æ»¤æ–‡ä»¶
            filtered_file = self.filter_continuous_metrics_file(file_path)
            if filtered_file:
                filtered_files.append(filtered_file)
                
                # ç»Ÿè®¡æŒ‡æ ‡æ•°é‡
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        original_data = json.load(f)
                    with open(filtered_file, 'r', encoding='utf-8') as f:
                        filtered_data = json.load(f)
                    
                    original_count = sum(len(dp.get('metrics', [])) for dp in original_data)
                    filtered_count = sum(len(dp.get('metrics', [])) for dp in filtered_data)
                    
                    total_original_metrics += original_count
                    total_filtered_metrics += filtered_count
                    
                except Exception as e:
                    self.console.print(f"[yellow]âš ï¸ ç»Ÿè®¡æŒ‡æ ‡æ•°é‡å¤±è´¥: {e}[/yellow]")
        
        # æ˜¾ç¤ºæ€»ä½“ç»Ÿè®¡
        removed_count = total_original_metrics - total_filtered_metrics
        self.console.print(f"[green]ğŸ‰ æ‰€æœ‰æ–‡ä»¶è¿‡æ»¤å®Œæˆ![/green]")
        self.console.print(f"[blue]ğŸ“Š æ€»ä½“ç»Ÿè®¡:[/blue]")
        self.console.print(f"[dim]  â€¢ å¤„ç†æ–‡ä»¶æ•°: {len(filtered_files)}[/dim]")
        self.console.print(f"[dim]  â€¢ åŸå§‹æŒ‡æ ‡æ€»æ•°: {total_original_metrics}[/dim]")
        self.console.print(f"[dim]  â€¢ è¿‡æ»¤åæŒ‡æ ‡æ•°: {total_filtered_metrics}[/dim]")
        self.console.print(f"[dim]  â€¢ ç§»é™¤æŒ‡æ ‡æ•°: {removed_count}[/dim]")
        self.console.print(f"[dim]  â€¢ è¿‡æ»¤åæ–‡ä»¶ä¿å­˜åœ¨: metrics/reports/filtered/[/dim]")
        
        return filtered_files

def main():
    """ä¸»å‡½æ•°"""
    filter_processor = TestSpecificFilter()
    
    # è‡ªåŠ¨è¿‡æ»¤æ‰€æœ‰æŒç»­æŒ‡æ ‡æ–‡ä»¶
    filtered_files = filter_processor.auto_filter_all_continuous_files()
    
    if filtered_files:
        console.print(f"[green]âœ… æˆåŠŸè¿‡æ»¤ {len(filtered_files)} ä¸ªæ–‡ä»¶[/green]")
        for file_path in filtered_files:
            console.print(f"[dim]  â€¢ {file_path}[/dim]")
    else:
        console.print("[yellow]âš ï¸ æ²¡æœ‰æ‰¾åˆ°éœ€è¦è¿‡æ»¤çš„æ–‡ä»¶[/yellow]")

if __name__ == "__main__":
    main()
